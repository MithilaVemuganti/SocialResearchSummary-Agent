# ğŸ§  Social Research Summary Agent

> An AI-powered agent that generates and evaluates research summaries in domains like climate, education, and social impact using IBM Granite models on Watsonx.ai.

---

## ğŸ“Œ Project Overview

Researchers and students often struggle to keep up with the growing volume of academic content. This agent helps streamline the early stages of academic research by generating simulated summaries and comparing them with real abstracts. It scores AI summaries based on **clarity**, **completeness**, and **coverage** â€” making it ideal for literature review, academic exploration, or educational feedback.

---

## ğŸš€ Features

- âœï¸ Summarizes research papers based on a title/topic
- ğŸ“Š Evaluates AI-generated summaries against real human-written abstracts
- ğŸ§  Benchmarks summaries for clarity, completeness, and coverage
- ğŸŒ Uses Google Search Tool for contextual information
- âš™ï¸ Built with IBM Granite and Watsonx Prompt Lab

---

## ğŸ’¡ Technologies Used

- **IBM Watsonx.ai Studio**
- **IBM Granite Foundation Model** (`granite-3-3b-instruct`)
- **Retrieval-Augmented Generation (RAG)**
- **Agent Lab** for custom instruction tuning
- **Google Search API Tool** (via IBM Agent Lab)

---

## ğŸ“š Use Cases

- ğŸ“„ Research summary generation
- ğŸ“š Literature review assistance
- ğŸ§‘â€ğŸ« Educational AI tutor or paper evaluation assistant
- ğŸ“Š Benchmarking AI-generated vs human-written summaries
- ğŸ›ï¸ Policy and NGO research support

---

## ğŸ‘¥ End Users

- ğŸ§‘â€ğŸ“ Students & Research Scholars  
- ğŸ§ª Academics & Educators  
- ğŸ›ï¸ NGOs, Think Tanks, and Policy Analysts  
- ğŸ“š Editors or Journal Submission Reviewers  

---

## âœ¨ Wow Factors

- ğŸ” **Semantic summarization** of complex research topics  
- ğŸ“ˆ **Scoring rubric** for evaluating AI vs. human-written abstracts  
- ğŸ§  **Agentic interaction** simulating real research workflows  
- ğŸ—‚ï¸ **Structured and readable** respo



